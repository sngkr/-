% !TeX root = ../main.tex

\chapter{需求分析与典型场景建模}
\label{chap:requirements}

本章首先通过对实际企业级软件交付过程的深入调研，提炼出几种典型的多变更域协同场景，深刻剖析其中蕴含的复杂依赖关系与痛点问题。在此基础上，利用形式化方法对多变更域持续交付问题进行建模，明确核心概念与逻辑关系。最后，结合场景分析与理论模型，详细阐述系统的功能性需求与非功能性需求，明确系统设计目标与约束条件。

\section{概述}

需求分析是系统设计的基石。在微服务架构与云原生技术普及的今天，软件交付早已超越了单纯的“代码发布”范畴，演变为涵盖应用代码、运行时配置、基础设施资源、数据存储模式等多个域的复杂系统工程。传统的持续交付系统多聚焦于单一应用流水线，难以有效应对跨域变更带来的协同挑战。

为了构建一套真正面向多变更域的持续交付体系，必须首先厘清多变更域场景下的核心矛盾。本章将从实际生产环境中的典型案例出发，识别不同变更域之间的耦合模式与风险传播路径，从而推导出系统所需的核心能力，如全链路依赖感知、跨域流水线编排、智能发布控制等，为后续的架构设计与系统实现提供坚实的依据。

\section{典型多变更域场景分析}

在现代分布式系统中，单一功能的上线往往牵一发而动全身。本节选取了四个最具代表性的多变更域协同场景，分析其变更流程、依赖关系及潜在风险。

\subsection{跨服务功能变更协同场景}

随着业务拆分的细化，一个用户请求往往需要经过多个微服务的协作才能完成。当业务需求发生变更时，通常需要多个服务同时修改接口或逻辑，形成了跨服务的变更依赖。

\textbf{场景描述}：
假设电商系统中新增了“预售商品”功能。该功能涉及以下服务变更：
\begin{enumerate}
    \item \textbf{商品服务（Product Service）}：需修改数据库Schema以支持“预售”标记，并提供新的API接口查询预售信息。
    \item \textbf{订单服务（Order Service）}：需并在下单流程中增加预售校验逻辑，调用商品服务的新接口。
    \item \textbf{库存服务（Inventory Service）}：需区分预售库存与现货库存，逻辑需做相应调整。
    \item \textbf{前端网关（API Gateway）}：需聚合新的预售字段并透出给前端。
\end{enumerate}

\textbf{变更依赖与挑战}：
此场景中存在严格的逻辑依赖与发布时序依赖：
\begin{itemize}
    \item \textbf{接口依赖}：订单服务依赖商品服务的新接口。若订单服务先上线，调用旧版商品服务将报错，导致下单失败。
    \item \textbf{数据依赖}：商品服务代码上线前，数据库Schema必须先完成变更，否则服务启动失败。
    \item \textbf{版本兼容性}：在灰度发布过程中，新旧版本服务可能共存。例如，新版订单服务请求打到了旧版商品服务节点，必须有降级策略或路由控制，否则会导致请求异常。
\end{itemize}

\textbf{痛点分析}：
在现有体系下，各服务往往由不同团队维护，拥有独立的流水线。协调这些服务的发布顺序全靠人工沟通（如飞书群、Excel表），极易出现“漏发”、“错序”或“未对齐灰度比例”的情况，导致线上故障。

\subsection{配置驱动的业务逻辑变更场景}

配置中心（Configuration Center）使得应用程序可以在不重新部署代码的情况下调整运行时行为。然而，配置变更并非总是无风险的，配置与代码之间存在着隐性的强耦合。

\textbf{场景描述}：
某推荐系统计划上线一种新的“协同过滤算法”。
\begin{enumerate}
    \item \textbf{代码变更}：算法团队开发了新的推荐模型代码，并已合入主干，随常规版本发布。
    \item \textbf{配置变更}：运维人员需要在配置中心新增一个开关 \texttt{enable\_collaborative\_filtering = true}，并配置算法参数 \texttt{model\_path} 和 \texttt{timeout\_ms}。
\end{enumerate}

\textbf{变更依赖与挑战}：
\begin{itemize}
    \item \textbf{时序依赖}：代码必须先于配置上线。如果配置先下发，而代码中尚无处理该配置的逻辑（或者旧版本代码对新配置项解析异常），可能导致服务崩溃或行为不可预期。
    \item \textbf{值域依赖}：配置值的合法性依赖于代码实现。例如，代码中对 \texttt{timeout\_ms} 设定的范围是 0-1000ms，若配置误配为 5000ms，可能引发线程池耗尽。
    \item \textbf{全量风险}：配置下发通常是毫秒级全量生效的，一旦配置错误，所有实例同时挂掉，不仅无法通过常规的金丝雀发布规避，甚至会导致全站雪崩。
\end{itemize}

\textbf{痛点分析}：
目前的CI/CD工具常将代码流水线与配置管理隔离。代码发布有层层测试把关，而配置修改往往是运维人员在控制台“点一下”，缺乏自动化测试与依赖校验。系统无法识别“该配置项需要最低版本v1.2.0支持”，极易引发因版本不匹配导致的事故。

\subsection{基础设施升级与应用适配场景}

基础设施即代码（IaC）使得底层资源的变更也变得频繁。基础设施的升级（如Kubernetes版本升级、中间件替换）往往对上层应用产生深远影响。

\textbf{场景描述}：
平台团队计划将Kubernetes集群从v1.22升级至v1.25，同时废弃旧版Ingress API (\texttt{networking.k8s.io/v1beta1})，强制迁移到v1版本。

\textbf{变更依赖与挑战}：
\begin{itemize}
    \item \textbf{API兼容性依赖}：所有应用服务的Helm Chart或YAML文件必须修改，将Ingress定义升级为新版API。
    \item \textbf{环境依赖}：应用代码可能依赖特定版本的JDK基础镜像或Sidecar版本。基础设施升级基础镜像后，应用可能因类库冲突无法启动。
    \item \textbf{规模效应}：基础设施变更通常涉及成百上千个应用。如何确保所有应用都已完成适配？如果强制升级，未适配的应用将无法部署；如果逐个通知，沟通成本极高。
\end{itemize}

\textbf{痛点分析}：
基础设施层与应用层处于割裂状态。平台团队难以精准评估一个底层变更会影响哪些上层应用（例如，哪些应用还在使用旧版API）。通常采用“人肉排查”或“停机演练”的方式，效率低下且风险不可控。

\subsection{数据模式演进与代码同步场景}

数据是系统的核心资产。在业务快速迭代中，数据库Schema（DDL）的变更极其频繁，且回滚代价巨大。

\textbf{场景描述}：
用户中心服务需要将 \texttt{user\_id} 字段从 \texttt{int} 类型扩展为 \texttt{bigint} 以支撑用户量增长。

\textbf{变更依赖与挑战}：
\begin{itemize}
    \item \textbf{破坏性变更}：修改主键类型是破坏性变更（Breaking Change）。
    \item \textbf{代码-数据强一致性}：旧版代码只能处理 \texttt{int} 类型，新版代码才能处理 \texttt{bigint}。在发布过程中，若数据库先变更为 \texttt{bigint}，旧版代码读取数据时可能发生溢出或类型转换错误。
    \item \textbf{不可逆性}：一旦数据写入了超过 \texttt{int} 上限的数值，数据库就无法简单的回滚到 \texttt{int} 类型Schema。这要求发布策略必须极其谨慎，通常采用“双写过渡”方案。
\end{itemize}

\textbf{痛点分析}：
DDL变更通常由DBA手动执行或通过独立的工单系统流转，与应用发布流水线脱节。系统缺乏对“数据库变更-应用发布”这一原子操作的编排能力，难以实现自动化的双写迁移或停机发布控制。

\section{多变更域问题建模}

为了科学地解决上述复杂场景中的协同问题，我们需要将非结构化的业务问题转化为结构化的数学模型。本节定义核心概念，并构建变更传播模型。

\subsection{核心概念定义}

\begin{definition}[变更域 (Change Domain)]
变更域是指在软件交付过程中，具有独立版本控制、独立生命周期管理且逻辑上内聚的实体集合。常见的变更域 $D$ 包括：
\begin{itemize}
    \item \textbf{应用域 ($D_{app}$)}：包含源代码、编译产物、容器镜像等。
    \item \textbf{配置域 ($D_{conf}$)}：包含环境变量、启动参数、特征开关、动态配置等。
    \item \textbf{设施域 ($D_{infra}$)}：包含Kubernetes资源（Deployment/Service）、虚拟机、存储卷、网络策略等。
    \item \textbf{数据域 ($D_{data}$)}：包含数据库Schema、索引、初始数据、消息队列Topic定义等。
\end{itemize}
\end{definition}

\begin{definition}[变更原子 (Change Atom)]
变更原子 $c$ 是指发生在一个变更域内的最小不可分割修改单元。一个变更请求 $R$ 通常由多个变更原子组成，即 $R = \{c_1, c_2, ..., c_n\}$，其中 $c_i \in D_j$。
例如，一个功能上线可能包含：修改应用代码 ($c_{app}$)、新增配置项 ($c_{conf}$) 和修改数据表结构 ($c_{data}$)。
\end{definition}

\begin{definition}[依赖关系 (Dependency)]
对于两个变更原子 $c_i$ 和 $c_j$，若 $c_i$ 的正确执行或生效前提是 $c_j$ 处于特定状态，则称 $c_i$ 依赖于 $c_j$，记作 $c_i \to c_j$。
依赖关系具有多种类型：
\begin{itemize}
    \item \textbf{时序依赖 (Sequential)}：$c_j$ 必须在 $c_i$ 之前完成执行（如 DDL $\to$ 代码部署）。
    \item \textbf{值域依赖 (Value)}：$c_i$ 的取值必须在 $c_j$ 定义的范围内（如 配置值 $\to$ 代码逻辑）。
    \item \textbf{共存依赖 (Co-existence)}：$c_i$ 和 $c_j$ 必须同时存在或同时不存在（如 Service $\to$ Deployment）。
\end{itemize}
\end{definition}

\subsection{依赖关系图模型}

基于上述定义，整个系统的状态空间可以建模为一个有向图 $G = (V, E)$。

\begin{itemize}
    \item \textbf{顶点集 $V$}：代表系统中的所有实体（服务、配置项、资源对象）。
    \item \textbf{边集 $E$}：代表实体间的依赖关系。
\end{itemize}

在持续交付过程中，我们将“变更”视为对图 $G$ 的扰动。构建 \textbf{多维依赖图 (Multi-Dimensional Dependency Graph, MDDG)} 是解决问题的关键。MDDG 不同于传统的静态调用图，它是一个异构图（Heterogeneous Graph）。

$$ MDDG = \{ G_{app}, G_{conf}, G_{infra}, E_{cross} \} $$

其中，$G_{app}$ 是应用间的调用图，$G_{conf}$ 是配置与应用的映射图，$G_{infra}$ 是设施拓扑图，$E_{cross}$ 是跨域关联边。例如，一个Pod节点（设施）运行了某个镜像（应用），挂载了某个ConfigMap（配置），连接了某个Service（设施）。

\subsection{变更风险传播模型}

基于MDDG，我们可以定义变更风险的传播路径。
设初始变更集合为 $C_{init}$，风险传播函数为 $P(v)$，表示节点 $v$ 受影响的概率或严重程度。

$$ P(v) = \alpha \cdot I(v) + \beta \cdot \sum_{u \in In(v)} w(u, v) \cdot P(u) $$

其中：
\begin{itemize}
    \item $I(v)$：节点本身的内在风险（如核心服务比边缘服务风险大）。
    \item $In(v)$：所有指向 $v$ 的前驱节点集合（即依赖 $v$ 的节点）。
    \item $w(u, v)$：依赖强度权重。强依赖（如RPC调用）权重高，弱依赖（如异步消息）权重低。
    \item $\alpha, \beta$：调节系数。
\end{itemize}

该模型揭示了风险传播的“蝴蝶效应”：底层的微小变更（如公共类库升级、基础镜像漏洞修复）可能沿着依赖链向上传导，最终导致大量顶层业务服务的不稳定。

\section{系统功能性需求}

基于典型场景分析与理论建模，本研究提出的多变更域持续交付系统需具备以下四大核心功能模块。

\subsection{多维度依赖自动分析}

系统必须具备自动化构建 MDDG 的能力，摒弃人工维护依赖关系的落后方式。

\begin{enumerate}
    \item \textbf{静态解析能力}：能够解析代码（Java/Go import）、配置（Spring @Value, YAML）、设施（K8s Manifest, Terraform）文件，提取显式依赖。
    \item \textbf{动态追踪能力}：集成APM（如SkyWalking、Jaeger）数据，捕获运行时RPC调用链，识别静态分析无法发现的动态依赖或条件依赖。
    \item \textbf{跨域关联能力}：能够将不同域的实体关联起来。例如，通过解析 Deployment YAML 将 Service 与 ConfigMap 关联；通过解析 datasource 配置将 Application 与 Database 关联。
    \item \textbf{影响域计算}：给定一个变更请求（如修改配置项A），系统能快速计算出“受影响半径”，输出受影响的服务列表及风险等级报告。
\end{enumerate}

\subsection{跨域流水线编排}

传统的单体流水线已无法满足需求，系统需支持分层、模块化的流水线编排。

\begin{enumerate}
    \item \textbf{层级化模型}：支持“变更原子流水线”（构建、部署单个组件）与“业务发布流水线”（编排多个原子流水线）。
    \item \textbf{依赖驱动的调度}：流水线引擎应具备DAG（有向无环图）调度能力，根据变更原子间的时序依赖关系（由前述模型计算得出）自动生成执行计划。例如，自动安排先执行DDL变更，再并行部署三个微服务。
    \item \textbf{原子性与事务性}：支持跨域变更的原子性。如果中间某个环节失败（如ConfigMap更新成功但Pod重启失败），能够触发自动回滚流程，将所有相关域恢复至变更前状态。
\end{enumerate}

\subsection{依赖感知的发布控制}

发布过程必须是智能的、灰度的、可观测的。

\begin{enumerate}
    \item \textbf{多级灰度策略}：支持基于流量（Header/Cookie）、地域、用户ID的精细化流量切分。灰度不仅作用于应用流量，也应尽可能作用于配置生效范围。
    \item \textbf{协同发布网关}：提供统一的发布控制平面，协调不同服务的灰度比例。例如，保证服务A的灰度版本只调用服务B的灰度版本（全链路灰度），防止环境串扰。
    \item \textbf{发布门禁（Quality Gates）}：在发布的每个阶段（Canary 1\% -> 10\% -> 100%）设置自动化检查点，检查指标（Error Rate, Latency）是否超标，依赖约束是否满足。
\end{enumerate}

\subsection{全链路变更监控与回溯}

\begin{enumerate}
    \item \textbf{统一变更事件流}：收集所有域的变更事件（Git Commit, K8s Event, Config Change, SQL Execution）至统一的时间序列数据库。
    \item \textbf{变更与故障关联}：当系统出现故障时，能迅速在时间轴上定位最近发生的变更事件，并结合依赖图推荐可能的“嫌疑变更”。
    \item \textbf{配置漂移检测}：定期扫描实际运行环境与Git仓库的差异，发现并告警“有人在生产环境直接修改了配置但未提交代码”的违规行为。
\end{enumerate}

\section{系统非功能性需求}

除了功能完善，作为企业级核心基础设施，系统还必须满足严格的非功能性指标。

\subsection{可扩展性 (Scalability)}
\begin{itemize}
    \item \textbf{接入能力}：系统应能支撑数千个微服务、数万个配置项的依赖分析，依赖图构建算法的时间复杂度应控制在合理范围（如 $O(N \log N)$）。
    \item \textbf{插件化架构}：适应异构技术栈。不同的编程语言（Java/Go/Node）、不同的基础设施（K8s/VM/Serverless）应能通过插件方式轻松接入，无需修改核心代码。
\end{itemize}

\subsection{高可用性 (Availability)}
持续交付系统是研发效能的“生命线”。如果发布系统宕机，将导致无法修复线上紧急Bug。
\begin{itemize}
    \item \textbf{系统自身高可用}：核心组件需集群部署，支持多活容灾。
    \item \textbf{故障降级}：在依赖分析服务不可用时，应允许降级为人工审批模式继续发布，而不是完全阻断发布流程。
\end{itemize}

\subsection{安全性 (Security)}
\begin{itemize}
    \item \textbf{权限隔离}：支持基于RBAC（Role-Based Access Control）的细粒度权限控制。开发人员只能变更自己服务的代码，不能随意修改生产环境的基础设施或敏感配置。
    \item \textbf{审计合规}：所有变更操作必须留痕，满足金融级审计要求。记录“谁、在什么时间、修改了哪个域的什么内容、审批人是谁”。
    \item \textbf{秘钥管理}：敏感配置（如DB密码）必须加密存储与传输，严禁明文出现在流水线日志中。
\end{itemize}

\subsection{易用性 (Usability)}
\begin{itemize}
    \item \textbf{开发者体验}：提供可视化的依赖视图与发布大盘，让复杂的依赖关系“看得见、摸得着”。
    \item \textbf{声明式配置}：尽可能采用声明式（Declarative）而非指令式（Imperative）的交互方式，降低用户的学习成本与操作负担。
\end{itemize}

\section{本章小结}

本章深入探讨了多变更域持续交付面临的现实挑战。通过对跨服务、配置、基础设施、数据等典型协同场景的剖析，我们揭示了现有单体交付体系在应对复杂依赖时的局限性。随后，我们建立了包含变更域、依赖关系图、风险传播在内的理论模型，为问题的求解提供了数学工具。

在此基础上，我们明确了系统的功能性需求，重点在于“多维依赖分析”、“跨域编排”、“智能发布”与“全链路监控”四大支柱；同时界定了性能、安全等非功能性约束。这些分析结论为下一章的系统详细架构设计指明了方向，奠定了坚实的理论基础和工程目标。

% End of requirements.tex
